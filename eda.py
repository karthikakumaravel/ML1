# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17eSq9u0wc8HgszETzQm99Uaszq45FE24
"""

import pandas as pd
import numpy as np

path="https://raw.githubusercontent.com/nunnarilabs/ml/master/Automobile_Data/automobile_data.csv"
data=pd.read_csv(path)
data.head()

#replace question mark with null
#inplace replace with original dataset
data.replace("?",np.nan,inplace=True)
data.head(5)

data.isnull().sum()

data.shape

data.columns.tolist()

data.describe()

data.info()

data.tail(10)#last 10 rows if i want to view

data.head(10)

data.drop_duplicate(inplace=True)

data.shape

#isnull     notnull

#drop-whole row drop,column drop mostly drop row
#replace- mean,frequency,sumvalue based on some values

avg_norm_loss=data["normalized-losses"].astype("float").mean(axis=0)
print(avg_norm_loss)

data["normalized-losses"].replace(np.nan,avg_norm_loss,inplace=True)

avg_bore=data['bore'].astype('float').mean(axis=0) #axis=0 rows axis=1 column
print(avg_bore)

data['num-of-doors'].value_counts()

data['num-of-doors'].replace(np.nan,"Four",inplace=True)
data

data.dropna(subset=["price"],axis=0,inplace=True)
data

data.reset_index(drop=True,inplace=True)
data

data.isnull().sum()

data.nunique()

data['stroke'].replace(np.nan,data['stroke'].astype('float').mean(axis=0),inplace=True)
data['horsepower'].replace(np.nan,data['horsepower'].astype('float').mean(axis=0),inplace=True)

data.isnull().sum()

data['bore'].replace(np.nan,data['bore'].astype('float').mean(axis=0),inplace=True)
data['peak-rpm'].replace(np.nan,data['peak-rpm'].astype('float').mean(axis=0),inplace=True)

data.isnull().sum()

data[["bore","stroke"]]=data[["bore","stroke"]].astype("float")
data[["normalized-losses"]]=data[["normalized-losses"]].astype("int")
data[["price"]]=data[["price"]].astype("float")
data[["peak-rpm"]]=data[["peak-rpm"]].astype("float")
data[["horsepower"]]=data[["horsepower"]].astype("float")
data

from scipy import stats
hp_zscore=np.abs(stats.zscore(data['normalized-losses']))
hp_zscore

threshold=hp_zscore.max()
index_val=np.where(hp_zscore >= threshold)
data.iloc[index_val]['normalized-losses']

#drop the row that has outliers

newdata=data.drop(index=data.iloc[index_val]["normalized-losses"].index)
newdata

#calculate upper and lower limits



Q1=data['length'].quantile(0.25)
Q3=data['length'].quantile(0.75)
IQR=Q3-Q1
lower=Q1-1.5*IQR
upper=Q3+1.5*IQR

data.describe(include=['object'])

b=data['drive-wheels'].value_counts()
print(b)

a=data['drive-wheels'].value_counts().to_frame()
print(a)

d_w_c=data['drive-wheels'].value_counts().to_frame()
d_w_c.rename(columns={'drive-wheels':'value_counts'},inplace=True)
d_w_c.index.name='drive-wheels'
print(d_w_c)

data.columns=data.columns.str.replace('-','_')
data

data['drive_wheels'].unique()

data['body_style'].unique()

group_one=data[['drive_wheels','price']]
group_one.head(5)

#groupingg
df_grptest=data[['drive_wheels','body_style','price']]
grptest1=df_grptest.groupby(['drive_wheels','body_style'],as_index=False).mean()
grptest1

grp_piot=grouped_test1.pivot()index

grouped

#zscore
originalval-mean/(standard deviation)

ftest= based on variance btwn two table
variance = hw long it has been distanced itself from mean

data['city_1/10km']=235/data['city_mpg']

data

from sklearn.preprocessing import StandardScaler

data1=[[1,2],[3,4],[5,6]]

scaler=StandardScaler()

standardized_data=scaler.fit_transform(data1)

print("original data")
print(data1)

print("standardized data")
print(standardized_data)

data_array = np.array(data1)
first_column = data_array[:, 0]
mean = np.mean(first_column)
std_dev = np.std(first_column)
print(mean)
print(std_dev)

second_column = data_array[:, 1]
mean = np.mean(second_column)
std_dev = np.std(second_column)
print(mean)
print(std_dev)

data['length']=data['length']/data['length'].max()
data['width']=data['width']/data['width'].max()
data['height']=data['height']/data['height'].max()

data[['length','width','height']]

#min max scale
from sklearn.preprocessing import MinMaxScaler

data1=[[1,2],[3,4],[5,6]]#creating simple 2d datset
scaler=MinMaxScaler()
standardized_data=scaler.fit_transform(data1)
print("Original data: ")
print(data1)
print("\nStandardized Data: ")
print(standardized_data)

